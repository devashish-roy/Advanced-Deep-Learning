{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3593,
     "status": "ok",
     "timestamp": 1714689662543,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "UAraRs_LoSl9",
    "outputId": "16975e81-cac4-4fad-d516-58a31bb20cc8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')         # only needed first time\n",
    "nltk.download('stopwords')     # only needed first time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n",
    "from copy import deepcopy\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1999,
     "status": "ok",
     "timestamp": 1714689666962,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "0p9hLbtb-sQJ",
    "outputId": "6459dc39-bca6-42bc-a8d4-e5ae6b951dc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1714690576811,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "_zelQWQp9xpA"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "#batch_size = 900 # how many independent sequences will we process in parallel?\n",
    "block_size = 149 # what is the maximum context length for predictions? This is decided based on value in max_length\n",
    "max_iters = 20\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 20\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "num_classes = 2\n",
    "PATH_MODEL = '/content/drive/My Drive/Models/model_transformer_sentiment_batched.pth'\n",
    "best_val_loss = float('inf')\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1714689672727,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "6dg9QqXUZWPL"
   },
   "outputs": [],
   "source": [
    "#Need some more work to implement multiple files\n",
    "\"\"\"\n",
    "# Read all the *labelled.txt files in the curent directory\n",
    "dfs = []\n",
    "\n",
    "# Specify the file path the Google Drive\n",
    "directory_path = '/content/drive/My Drive/Data/'\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('labelled.txt'):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        # Read CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path, delimiter='\\t', header=None)\n",
    "        # Append DataFrame to list\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1714690338207,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "W6VcXN6W9xsi"
   },
   "outputs": [],
   "source": [
    "# Specify the file path the Google Drive\n",
    "file_path = '/content/drive/My Drive/Data/amazon_cells_labelled.txt'\n",
    "\n",
    "# Read the file using pandas\n",
    "data = pd.read_csv(file_path, delimiter='\\t', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1714689677091,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "JcbY551p9xwK"
   },
   "outputs": [],
   "source": [
    "def preprocess_pandas(data, columns):\n",
    "    df_ = pd.DataFrame(columns=columns)\n",
    "    data['Sentence'] = data['Sentence'].str.lower()\n",
    "    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n",
    "    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP address\n",
    "    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]','')                                                       # remove special characters\n",
    "    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)                                                   # remove numbers\n",
    "    for index, row in data.iterrows():\n",
    "        word_tokens = word_tokenize(row['Sentence'])\n",
    "        filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n",
    "        df_ = df_._append({\n",
    "            \"index\": row['index'],\n",
    "            \"Class\": row['Class'],\n",
    "            \"Sentence\": \" \".join(filtered_sent[0:])\n",
    "        }, ignore_index=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 3004,
     "status": "ok",
     "timestamp": 1714690350631,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "oCQri4Hd9x2Z"
   },
   "outputs": [],
   "source": [
    "data.columns = ['Sentence', 'Class']\n",
    "data['index'] = data.index                                          # add new column index\n",
    "columns = ['index', 'Class', 'Sentence']\n",
    "data = preprocess_pandas(data, columns)                             # pre-process\n",
    "\n",
    "training_data, validation_data, training_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
    "    data['Sentence'].values.astype('U'),\n",
    "    data['Class'].values.astype('int32'),\n",
    "    test_size=0.10,\n",
    "    random_state=0,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_data, validation_data, test_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
    "    validation_data,\n",
    "    validation_labels,\n",
    "    test_size=0.50,\n",
    "    random_state=0,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1714690353026,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "SThB_O4r9x5q"
   },
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "concatenated_string = data['Sentence'].str.cat()\n",
    "chars = sorted(list(set(concatenated_string)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1714689698027,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "129L2F4_9x8l"
   },
   "outputs": [],
   "source": [
    "def encode_text(text):\n",
    "    if isinstance(text, str):  # Check if input is a single string\n",
    "        return [stoi[c] for c in text]\n",
    "    elif isinstance(text, np.ndarray):  # Check if input is a NumPy array\n",
    "        return [encode_text(s) for s in text]\n",
    "    elif isinstance(text, list):  # Check if input is a list of strings\n",
    "        return [encode_text(s) for s in text]\n",
    "    else:\n",
    "        raise TypeError(\"Input must be a string or a list of strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1714690359418,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "iH8QSnl_9x_U",
    "outputId": "f178ef06-8ac3-42b3-d999-d08b032e2eef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !\"#$%&'()*+,-./:;?[]abcdefghijklmnopqrstuvwxyz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(''.join(chars))\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1714689706930,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "b8Suha0WHBZv"
   },
   "outputs": [],
   "source": [
    "# Define a custom dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1714690365021,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "eQX2Zjh09yCr"
   },
   "outputs": [],
   "source": [
    "#encode training data\n",
    "encoded_training_data = encode_text([training_data[i] for i in range(len(training_data))])\n",
    "\n",
    "# Determine the maximum length\n",
    "max_length = max(len(seq) for seq in encoded_training_data)\n",
    "\n",
    "# Pad or truncate per maximum length\n",
    "padded_training_data = [seq + [0] * (max_length - len(seq)) for seq in encoded_training_data]\n",
    "\n",
    "# Convert to a PyTorch tensor\n",
    "encoded_training_data_tensor = torch.tensor(padded_training_data, dtype=torch.long)\n",
    "\n",
    "# Convert labels to tensor\n",
    "training_labels = torch.tensor(training_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1714690367462,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "2tai-FhBfbya",
    "outputId": "7d0660b2-480a-463e-ee05-fe4648f4192d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length   # this decides block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1714690393669,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "4QW8WKnH9yFd"
   },
   "outputs": [],
   "source": [
    "#Encode validation data\n",
    "encoded_validation_data = encode_text([validation_data[i] for i in range(len(validation_data))])\n",
    "\n",
    "# Pad or truncate per maximum length\n",
    "padded_validation_data = [seq + [0] * (max_length - len(seq)) for seq in encoded_validation_data]\n",
    "\n",
    "# Convert to a PyTorch tensor\n",
    "encoded_validation_data_tensor = torch.tensor(padded_validation_data, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Convert labels to tensor\n",
    "validation_labels = torch.tensor(validation_labels, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1714690396789,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "PkZ-cFOlH16Y"
   },
   "outputs": [],
   "source": [
    "#Encode test data\n",
    "encoded_test_data = encode_text([test_data[i] for i in range(len(test_data))])\n",
    "\n",
    "# Pad or truncate per maximum length\n",
    "padded_test_data = [seq + [0] * (max_length - len(seq)) for seq in encoded_test_data]\n",
    "\n",
    "# Convert to a PyTorch tensor\n",
    "encoded_test_data_tensor = torch.tensor(padded_test_data, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Convert labels to tensor\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1714690399479,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "0VYnDG_xHrgJ"
   },
   "outputs": [],
   "source": [
    "# Create a dataset instance\n",
    "train_dataset = MyDataset(encoded_training_data_tensor, training_labels)\n",
    "val_dataset = MyDataset(encoded_validation_data_tensor, validation_labels)\n",
    "test_dataset = MyDataset(encoded_test_data_tensor, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1714690402117,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "xJrEuS25H4Lt"
   },
   "outputs": [],
   "source": [
    "# Hyperparams. Set these to reasonable values\n",
    "BATCH_SIZE = 50\n",
    "SHUFFLE = 2\n",
    "\n",
    "\n",
    "# Load data in the loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxTkCn2dUrK4"
   },
   "source": [
    "## Transformer adapted to do sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1714691126863,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "YqYbOZks9yIW"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(best_val_loss):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "\n",
    "            if split == 'train':\n",
    "              for inputs, label in train_loader:\n",
    "                X, Y = inputs, label\n",
    "\n",
    "                #X, Y = get_batch(split)\n",
    "                logits, _ = model(X)\n",
    "                loss = criterion(logits, Y)\n",
    "                losses[k] = loss.item()\n",
    "\n",
    "            else:\n",
    "              for inputs, label in val_loader:\n",
    "                X, Y = inputs, label\n",
    "\n",
    "                #X, Y = get_batch(split)\n",
    "                logits, _ = model(X)\n",
    "                loss = criterion(logits, Y)\n",
    "                losses[k] = loss.item()\n",
    "\n",
    "                # when best model , save it\n",
    "                if loss < best_val_loss:\n",
    "                    # save the best loss\n",
    "                    best_val_loss = loss\n",
    "                    # Save the model\n",
    "                    torch.save(model.state_dict(), PATH_MODEL)\n",
    "\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out, best_val_loss\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        #wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)    # commented this masking for sentiment analysis so that every token can communicate with every other token\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        #self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.sentiment_classifier = nn.Linear(n_embd, num_classes) # classfier layer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        #print(idx.shape)\n",
    "        B, T = idx.shape\n",
    "\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        #logits = self.lm_head(x) # (B,T,vocab_size)  # this is from nanogpt implementation (decoder - generates text)\n",
    "        logits = self.sentiment_classifier(x[:, -1, :])  # Use only the last output of the model\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    # Generate is not used for sentiment analysis.\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 354437,
     "status": "ok",
     "timestamp": 1714691487111,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "Z2Hn7hmw9yL2",
    "outputId": "7e468dd8-3fe8-43de-d165-9910328ca36b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21197 M parameters\n",
      "step 0: train loss 0.6990, val loss 0.7057\n",
      "step 10: train loss 0.5646, val loss 0.7290\n",
      "step 19: train loss 0.2335, val loss 1.2438\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_loss_prev = float('inf')\n",
    "best_val_loss_current = float('inf')\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses, best_val_loss_current = estimate_loss(best_val_loss_prev)\n",
    "        best_val_loss_prev = best_val_loss_current\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    for inputs, label in train_loader:\n",
    "      xb, yb = inputs, label\n",
    "\n",
    "      # evaluate the loss\n",
    "      logits, _ = model(xb)\n",
    "      loss = criterion(logits, yb)\n",
    "      optimizer.zero_grad(set_to_none=True)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "# Save the model\n",
    "#PATH = '/content/drive/My Drive/Models/model_transformer_sentiment.pth'\n",
    "#torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsLnWnnzTgGt"
   },
   "source": [
    "### Test the model on test set\n",
    "With 200 iteration - Result is 78% accuracy  \n",
    "With 20 iteration - Result is 46% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1714691718004,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "67A2Rrnn9yO8",
    "outputId": "188dd45e-83f9-4042-c2ee-c1bd07e46893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 46.0 %\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Hyperparams. Set these to reasonable values\n",
    "BATCH_SIZE = 50\n",
    "SHUFFLE = 2\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Load data in the loader\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# load the model\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "#PATH = '/content/drive/My Drive/Models/model_transformer_sentiment.pth'\n",
    "model.load_state_dict(torch.load(PATH_MODEL))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, label in test_loader:\n",
    "      xb, yb = inputs, label\n",
    "\n",
    "      # evaluate the loss\n",
    "      logits, _ = model(xb)\n",
    "      _, predicted = torch.max(logits.data,1)\n",
    "      total += yb.size(0)\n",
    "      correct += (predicted==yb).sum().item()\n",
    "\n",
    "accuracy = correct/total * 100\n",
    "print(f'Accuracy {accuracy} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArRpacs9Og1k"
   },
   "source": [
    "### Bot that accepts user input, analyses it and responds to the user based on it analysis. Bot uses previously trained model for its analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79258,
     "status": "ok",
     "timestamp": 1714691814317,
     "user": {
      "displayName": "Devashish Roy",
      "userId": "04831984994490105226"
     },
     "user_tz": -120
    },
    "id": "3JOhCayW9yUx",
    "outputId": "bc91846d-0d8d-4b9b-c74d-b0b7ee1aaf2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hello! How are you feeling today? I will tell you if it is positive or negative.\n",
      "User: good\n",
      "Bot: Positive\n",
      "User: bad\n",
      "Bot: Negative\n",
      "User: thanks\n",
      "Bot: Negative\n",
      "User: negative\n",
      "Bot: Positive\n",
      "User: oh god\n",
      "Bot: Positive\n",
      "User: mobile phone\n",
      "Bot: Negative\n",
      "User: no\n",
      "Bot: Negative\n",
      "User: why not\n",
      "Bot: Negative\n",
      "User: thanks\n",
      "Bot: Negative\n",
      "User: thank you\n",
      "Bot: Negative\n",
      "User: great\n",
      "Bot: Positive\n",
      "User: short\n",
      "Bot: Negative\n",
      "User: exit\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Define a function to get a response for a given prompt\n",
    "def get_response(inp,model):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output, _ = model(inp)\n",
    "        _, predicted = torch.max(output.data,1)\n",
    "    #print(output)\n",
    "    #print(predicted)\n",
    "\n",
    "    if predicted == torch.tensor([0]):\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "\n",
    "# Function to pre-process input data\n",
    "def preprocess_user_input(prompt):\n",
    "    # get data, pre-process\n",
    "    data = pd.DataFrame([[prompt, None]], columns=['Sentence', 'Class'])\n",
    "    data['index'] = data.index                                          # add new column index\n",
    "    columns = ['index', 'Class', 'Sentence']\n",
    "    data = preprocess_pandas(data, columns)\n",
    "    data['Sentence'].values.astype('U')\n",
    "\n",
    "\n",
    "\n",
    "    #encode user input\n",
    "    encoded_user_input = encode_text([data['Sentence'][i] for i in range(len(data['Sentence']))])\n",
    "\n",
    "    # Pad or truncate per maximum length\n",
    "    padded_user_input = [seq + [0] * (max_length - len(seq)) for seq in encoded_user_input]\n",
    "\n",
    "    # Convert to a PyTorch tensor\n",
    "    encoded_user_input_tensor = torch.tensor(padded_user_input, dtype=torch.long)\n",
    "\n",
    "    return encoded_user_input_tensor\n",
    "\n",
    "\n",
    "# Main function to run the chatbot\n",
    "def main():\n",
    "    print(\"Bot: Hello! How are you feeling today? I will tell you if it is positive or negative.\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Bot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Preprocess the input\n",
    "        inp_x_tensor = preprocess_user_input(user_input)\n",
    "        #print(inp_x_tensor)\n",
    "\n",
    "        # Load the model\n",
    "        model = BigramLanguageModel()\n",
    "        m = model.to(device)\n",
    "        PATH = '/content/drive/My Drive/Models/model_transformer_sentiment.pth'\n",
    "        model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "        # Use the trained model to get the bot response. Bot responds with its analysis indicating wheter user input is positive or negative\n",
    "        response = get_response(inp_x_tensor, model)\n",
    "        print(\"Bot:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNUNhlkmmLGV0Obo1Wc9kF5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
